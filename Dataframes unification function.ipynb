{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e830e4e-5271-4810-a4db-85690e8285fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Function to process Parquet files with schema unification\n",
    "def process_parquet_files(spark, path: str, target_schema: StructType) -> DataFrame:\n",
    "    # Read the directory \n",
    "    temp_df = spark.read.option(\"recursiveFileLookup\", \"true\").parquet(path)\n",
    "    \n",
    "    # Get the list of all Parquet files \n",
    "    files = temp_df.inputFiles()  # Returns a list of full file paths\n",
    "#     print(f\"Found files in {path}:\")\n",
    "#     print(files)\n",
    "\n",
    "    # Process each file individually\n",
    "    dataframes = []\n",
    "    for file in files:\n",
    "        print(f\"Processing file: {file}\")\n",
    "        \n",
    "        # Read the file \n",
    "        df = spark.read.parquet(file)\n",
    "\n",
    "        select_expressions = []\n",
    "        for field in target_schema:\n",
    "            column_name = field.name\n",
    "            column_type = field.dataType.simpleString()\n",
    "            \n",
    "            if column_name in df.columns:\n",
    "                # Если колонка существует - приводим к нужному типу\n",
    "                select_expressions.append(col(column_name).cast(column_type).alias(column_name))\n",
    "            else:\n",
    "                # Если колонки нет - создаем с null значениями\n",
    "                select_expressions.append(lit(None).cast(column_type).alias(column_name))\n",
    "\n",
    "        # Применяем все изменения одним select\n",
    "        df = df.select(*select_expressions)\n",
    "        dataframes.append(df)\n",
    "\n",
    "        # # Cast columns to match the target schema\n",
    "        # for field in target_schema:\n",
    "        #     column_name = field.name\n",
    "        #     column_type = field.dataType.simpleString()  \n",
    "            \n",
    "        #     # Check if the column exists in the file\n",
    "        #     if column_name in df.columns:\n",
    "        #         df = df.withColumn(column_name, col(column_name).cast(column_type))\n",
    "        #     else:\n",
    "        #         # Add missing columns with null values, cast to the expected type\n",
    "        #         df = df.withColumn(column_name, col(column_name).cast(column_type).alias(column_name))\n",
    "        \n",
    "        # # Add the unified DataFrame for the file to the list\n",
    "        # dataframes.append(df)\n",
    "\n",
    "    # Combine all DataFrames\n",
    "    final_df = dataframes[0]\n",
    "    for df in dataframes[1:]:\n",
    "        final_df = final_df.unionByName(df)  # Align columns by name\n",
    "    \n",
    "    return final_df"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Dataframes unification function",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
